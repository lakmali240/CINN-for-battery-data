{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakmali240/CINN-for-battery-data/blob/main/MA421Homework5problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you will write a python implementation of a neural network with one hidden layer for the binary classification. Your network takes an n-dimensional data point and produces a scalar z to be used for binary classification. \n",
        "\n",
        "You will test it on the half moon dataset. \n",
        "\n",
        "The sections marked NEW are those functions you need to fill in.\n",
        "\n",
        "First we import some libraries as before. "
      ],
      "metadata": {
        "id": "HnzRlaQaW6y1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szla9qyoPuqg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define some functions involved. Use the formulations that avoid overflows.  \n",
        "1. sigmoid function sigmoid(t)\n",
        "2. log of sigmoid(t), called log_sig(t)\n",
        "3. log of 1-sigmoid = 1/(1+e^t), called log_one_sig(t)\n",
        "4. cross-entropy loss function given the inputs of label y and prediction y_hat = sigmoid(z), where y, y_hat, and z are vectors of dimension N. (N = # of data points.) You should implement this function with z, rather than y_hat, as the input; namely, the loss function should be\n",
        "\n",
        "    loss = -y log(sigmoid(z)) - (1-y) log (1-sigmoid(z)) \n",
        "\n",
        "  where log(sigmoid(z)) and log (1-sigmoid(z)) should be computed by the functions log_sig(z) and log_one_sig(z) in parts 2 and 3."
      ],
      "metadata": {
        "id": "0byO4vq0Xcxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(t):\n",
        "    return 1.0/(1 + np.exp(-t))\n",
        "\n",
        "def loss(y, z):\n",
        "    # compute loss(y, sigmoid(z)) \n",
        "    # y --> label \n",
        "    # z --> model output\n",
        "\n",
        "    loss = np.mean(-y*(mylog(z)) - (1-y)*mylogminus(z))\n",
        "    return loss\n",
        "\n",
        "def mylog(t):\n",
        "    # compute log(sigmoid(t)) to be used in loss \n",
        "\n",
        "    y = 0*t\n",
        "    m=t.shape[0]\n",
        "    for i in range(m):\n",
        "      if t[i] < 0:\n",
        "        y[i] = t[i]-np.log(1+np.exp(t[i]))\n",
        "      else:\n",
        "        y[i] = -np.log(1+np.exp(-t[i]))\n",
        "\n",
        "    return y\n",
        "\n",
        "def mylogminus(t):\n",
        "    # compute log(1-sigmoid(t)) to be used in loss \n",
        "\n",
        "    y = 0*t\n",
        "    m=t.shape[0]\n",
        "    for i in range(m):\n",
        "      if t[i] < 0:\n",
        "        y[i] = -np.log(1+np.exp(t[i]))\n",
        "      else:\n",
        "        y[i] = -t[i]- np.log(1+np.exp(-t[i]))\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "kuzmD54GT9yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **New:** \n",
        " We define the ReLU function and its derivative ReLUprime"
      ],
      "metadata": {
        "id": "4vJsELDY3H4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(t):\n",
        "\n",
        "def ReLUprime(t):\n",
        "    # derivative of ReLU \n",
        "    "
      ],
      "metadata": {
        "id": "IZbO79WI3GLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New:** Define the neural network function with one hidden layer. Your function should take the data matrix X (N-by-n matrix) containing N input points and produce a vector of dimension N containing N output values, one for each data point for the binary classification. The function should also take as inputs the model parameters: \n",
        "\n",
        "output layer: w (n-dimensional weigth vector), b (bias), \n",
        "\n",
        "hidden layer: W_one (n_one-by-n matrix), and b_one (n_one dimensional vector). \n",
        "\n",
        "Use ReLU as the nonlinear activation function."
      ],
      "metadata": {
        "id": "WLulJqXcbEpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(w, b, W_one, b_one, X):\n",
        "    \n",
        "    # X --> N-by-n Input.\n",
        "    # z --> model output.\n",
        "    # w --> output layer weight.\n",
        "    # b --> output layer bias.\n",
        "    # W_one --> hidden layer weight.\n",
        "    # b_one --> hidden layer bias.\n",
        "\n",
        "    return z"
      ],
      "metadata": {
        "id": "eI9PNMZnhy0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New:** Define the function that generates the output of the neural network and the gradient of the cross-entropy loss using the forward and backward propagation algorithms. The forward propagation should be the same as your *model(w, b, W_one, b_one, X)*. \n",
        "\n",
        "Your function should take the data matrix X (N-by-n matrix), the label y (N-vector), and the model parameters (w, b, W_one, b_one) as inputs and produces network output z, the gradients dw, db, dW_one, db_one as outputs. "
      ],
      "metadata": {
        "id": "vv_xi0ajaEEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradients(w, b, W_one, b_one, X, y):\n",
        "    \n",
        "    # X --> N-by-n Input.\n",
        "    # z --> model output.\n",
        "    # w --> output layer weight.\n",
        "    # b --> output layer bias.\n",
        "    # W_one --> hidden layer weight.\n",
        "    # b_one --> hidden layer bias.\n",
        "    \n",
        "    return z, dw, db, dW_one, db_one\n",
        "    "
      ],
      "metadata": {
        "id": "I8KJF8lrZlFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New:** Write the function that train the neural network by the gradient descent algorithm using a fixed number of iteration (*iter*) and learning rate (*lr*). Your function should take *iter* and *lr* as well as the initial parameters, the input data X and the label y as the inputs. It produces new parameters as output. Also compute the loss value at each iteration and output the sequence of the loss.\n",
        "\n",
        "This code should be basically the same as your *train()* function before. The only difference is that you do not need to call the *model()* function, as the model output should have been computed as part of the *gradient()* function output."
      ],
      "metadata": {
        "id": "wgqML9T2cOqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(w, b, W_one, b_one, X, y, iter, lr):\n",
        "    \n",
        "   # X --> N-by-n Input.\n",
        "    # z --> model output.\n",
        "    # w --> output layer weight.\n",
        "    # b --> output layer bias.\n",
        "    # W_one --> hidden layer weight.\n",
        "    # b_one --> hidden layer bias.\n",
        "    # iter --> number of iterations \n",
        "    # lr --> Learning rate.\n",
        "        \n",
        " \n",
        "    return w, b, W_one, b_one, losses\n",
        "\n"
      ],
      "metadata": {
        "id": "6bIdE16li086"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, \n",
        "\n",
        "1. *predict()* produces class prediction label y_label (N-vector of 0 or 1) given the model output z. \n",
        "\n",
        "2. *accuracy()* computes the accuracy of prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGC-EjrzeHyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(output):\n",
        "\n",
        "    # output --> model output\n",
        "    # pred_class --> model class prediction\n",
        "\n",
        "    # Empty List to store predictions.\n",
        "    pred_class = []    \n",
        "    # if z >= 0 --> 1\n",
        "    # if z < 0 --> 0\n",
        "    pred_class = [1 if i > 0 else 0 for i in output]\n",
        "    \n",
        "    return np.array(pred_class)\n",
        "\n",
        "def accuracy(y, y_predict):\n",
        "    accuracy = np.sum(y == y_predict) / len(y)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "HChwCsuWf07D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New:** We test the code on the half moon dataset. The code below  uses l_one = 100 neurons in the hidden layer and the weights are initialized to be the stadard random normal distribution N(0,1). You should also try the Xavier Glorot and Kaiming He initializations. You should try at least 4 learning rates. Your best accuracy should well exceed 95% with a loss below 0.1."
      ],
      "metadata": {
        "id": "9HMg8ADJKYfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X_train, y_train = make_moons(n_samples=500, noise=0.1)\n",
        "X_test, y_test = make_moons(n_samples=1000, noise=0.1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
        "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")\n",
        "\n",
        "# initialization\n",
        "l_one=150\n",
        "n=X_train.shape[1]\n",
        "w=np.random.randn(l_one)\n",
        "b = 0\n",
        "W_one = np.random.randn(l_one, X_train.shape[1])\n",
        "b_one = 0*np.random.rand(l_one)\n",
        "\n",
        "# train\n",
        "w, b, W_one, b_one, losses = train(w, b, W_one, b_one, X_train, y_train, iter=1000, lr=0.1)\n",
        "\n",
        "# print results\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "\n",
        "#training accuracy \n",
        "z = model(w,b,W_one, b_one,X_train)\n",
        "acc = accuracy(np.squeeze(y_train), predict(np.squeeze(z)))  \n",
        "print(acc)\n",
        "\n",
        "# testing accuracy \n",
        "z = model(w,b,W_one, b_one,X_test)\n",
        "acc = accuracy(np.squeeze(y_test), predict(np.squeeze(z))) \n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "QTVQqQhLfpIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JveuumxlzW8L"
      }
    }
  ]
}